<!DOCTYPE html>
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>Radiok : le contrôle vocal</title>
  
  <link rel="stylesheet"
        href="//netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css">
  <link rel="stylesheet" href="css/radiok.css">
</head>
<body>
  <div class="container-fluid main">

    <div class="container">
      <div class="row">
        <div class="col-md-12">

          <h2>La configuration audio</h2>
          <p>
            Malheureusement la gestion du son sous linux est
            particulièrement pénible et surtout
            <a href=
               "http://tuxradar.com/content/how-it-works-linux-audio-explained">
              très compliquée</a>. Cet aspect de
              l'application <em class="rk">RadioK</em> est celui qui
              demande le plus d'efforts pour obtenir un résultat
              satisfaisant. La contrainte à respecter pour établir la
              configuration audio était de rester le plus simple
              possible tout en permettant de gérer des haut-parleurs
              ou des casques, différents types de micro, branchés en
              usb ou non et de rester homogène sur la raspberry et sur
              des desktops. Pour tenir cette contrainte, sans
              s'arracher les cheveux, on ne peut guère faire
              l'économie de la lecture et de la compréhension de la
              documentation de base sur le sujet.
          </p>
          <p>
            La première chose à faire consiste à vérifier que la
            sortie audio fonctionne correctement sur la rpi. On peut
            tester par exemple dans l'ordre :
            <div class="code">
              <br>aplay /usr/share/sounds/purple/login.wav
              <br>speaker-test -c2 -twav
              <br>mplayer un_fichier.mp3
            </div>
            Si tout va bien on devrait entendre des sons. On peut
            (on doit ?) googler quelquechose comme <i>raspberry
            audio</i> pour trouver de la doc. Une des pages la moins inutile est
            <a href=
               "http://jeffskinnerbox.wordpress.com/2012/11/15/getting-audio-out-working-on-the-raspberry-pi/">celle-ci</a>.
         </p>
          <p>
            Il faut également vérifier que l'entrée audio est
            correctement prise en compte. Pour ce faire on peut
            utiliser ce genre de commande :
            <div class="code">
              arecord -d 10 -t wav ma_voix.wav
            </div>
            En parlant pendant 10 secondes devant le micro on fabrique
            ainsi un fichier <code>ma_voix.wav</code> que l'on doit
            pourvoir écouter ensuite avec <code>aplay</code>.
          </p>
          <p>
            Pour régler les niveaux de sortie et d'entrée on peut
            utiliser la commande <code>alsamixer(1)</code> depuis un
            terminal <code>xterm</code>. L'interface est des plus
            frustes mais elle permet de définir les volumes des
            haut-parleurs et du micro.
          </p>
          
          <p>
            La solution pour avoir du son sur les haut-parleurs sans
            utiliser la sortie hdmi et pour pouvoir brancher un micro 
            sur la rpi consiste à utiliser une carte son comme la 
            <a target="window.open('','matos')"
               href="http://www.amazon.fr/gp/product/B0037AOUUQ">
              Logilink 250743</a>. L'installation ne pose
            guère de problème car le module driver est disponible dans
            les distributions linux.
          </p>

          <p>
            La difficulté consiste à bien identifier le nom
            du <i>control</i> pour modifier le volume de sortie et
            utiliser le même nom - <code>PCM</code> - sur différents
            matériels de manière à avoir les scripts pour trouver la
            valeur du volume ou pour la changer qui soient identiques sur
            différentes machines.<br>
            Ces scripts qui encapsulent <code>amixer get PCM</code> et
            <code>amixer set PCM</code> sont ainsi portables sans
            modification. Mais sur la rpi avec certaines cartes son il
            arrive que <code>PCM</code> n'existe pas. Il faut alors
            créer un fichier <code>~/.asoundrc</code> pour configurer
            la bibliothèque
            <a href="http://www.alsa-project.org/"><code>alsa</code></a>.
            La syntaxe des fichiers de configuration alsa est
            particulièrement imbittable. Ce fichier produit l'effet
            escompté :
            <pre style="width: 25%; margin-left: 3em; margin-bottom:  1em;">
pcm.softvol {
  type softvol
  slave {
     pcm "cards.pcm.default"
  }
  control {
     name  "PCM"
     card  0
  }
}
pcm.!default {
  type  plug
  slave.pcm "softvol"
}
            </pre>
            Le contenu de ce fichier est décrit sur
            <a href=
"http://alsa.opensrc.org/How_to_use_softvol_to_control_the_master_volume">
              cette page</a>.
         </p>

          <h2>La première implémentation</h2>
          <p>
            Une première implémentation a été expérimentée. Elle se
            base sur les routines disponibles dans la bibliothèque
            <a href="http://cmusphinx.sourceforge.net/">pocket
            sphinx</a>. Le programme réalisé s'exécute localement sur
            le processeur de la machine, et il fonctionne en tentant
            de déchiffrer des mots prononcés en anglais.
          </p>
          <p>
            Le principe de fonctionnement est le suivant : une liste
            de mots à reconnaître est préparée, une fois compilée elle
            est fournie en paramètre du programme de reconnaissance
            vocale. Celui-ci prend biensûr aussi en entrée le signal
            provenant du micro. Il attend en continu les paroles
            prononcées devant le micro et génère la transcription en chaîne de
            caractères des mots reconnus. Ces mots sont transmis au
            server http qui les utilisent pour commander la radio. Les
            fichiers de cette implémentation sont dans le répertoire
            <code>RADIOK_HOME/vox/ps</code>.
          </p>
          <p>
            La liste des mots est stockée dans le fichier 
            <code>corpus-en.txt</code>. La version actuelle du
            programme travaille en anglais, il y aura peut-être par la
            suite une version comprenant le français. La liste
            proposée contient plus de mots que nécessaire mais ça
            permet d'expérimenter et de choisir ensuite le vocabulaire
            le plus efficace. La liste doit être compilée par le
            progamme <a href=
            "http://www.speech.cs.cmu.edu/tools/lmtool-new.html">
              <code>lmtool</code></a>. Les fichiers produits portant des
            noms comme <code>nnnn.lm</code> et <code>nnnn.dic</code>
            sont fournis en paramètres du programme de
            reconnaissance. À chaque fois que le corpus est modifié
            il faut recompiler la liste. On obtient alors de
            nouveaux fichiers <code>nnnn.*</code>.
            Ce nom - <code>nnnn</code> - est assigné à la variable
            <code>corpus</code>
            dans les scripts <code>listen.sh</code> et
            <code>trywords.sh</code> il faut donc les mettre à jour.
          </p>

          <p>
            Le programme de reconnaissance
            s'appelle <code>whatusay</code> (<i>what you say
            ?</i>). Le fichier source est <code>whatusay.c</code>. Il
            s'agit tout simplement d'une version simplifiée de 
            <a href="http://cmusphinx.sourceforge.net/">
            <code>pocketsphinx</code></a>. Une grande partie du code
            qui n'était pas pertinent
            pour <em class="rk">RadioK</em> a été purement et
            simplement supprimée. Inversement du code nécessaire à
            la communication avec le server web a été ajouté. Ce
            code s'appuie sur la bibliothèque
            <a href="http://curl.haxx.se/"><code>curl</code></a>. Chaque
            mot décodé est transmis par une requête http au server.
          </p>
          <p>
            Le programme <code>whatusay</code> accepte un certain
            nombre d'arguments dont les 2 principaux utilisés pour
            <em class="rk">RadioK</em> sont <code>-adcdev</code> et
            <code>-url</code>. Le premier <code>-adcdev</code> permet
            de spécifier le nom du device d'entrée. Là aussi il faut
            faire un effort pour comprendre quelle valeur
            mettre. Normalement quelque chose comme <code>hw:n</code>
            avec <code>n</code> égal à l'indice du device d'entrée (0 ou 1 ou
            plus) doit marcher. Le deuxième argument, spécifique à 
            <em class="rk">RadioK</em>, <code>-url</code> permet de
            définir l'url du serveur auquel envoyer les commandes
            interprétées par <code>whatusay</code>.
          </p> 
          <p>
            Pour refabriquer le programme il faut installer
            <code>cmusphinx</code> et <code>curl</code> puis lancer
            <code>make</code> dans le répertoire
            <code>RADIOK_HOME/vox/ps</code>.
            Le <code>Makefile</code> a été gardé délibérément très
            simple. Il doit éventuellement être modifié en
            fonction de la plateforme.
          </p>
          <p>
            Pour tester le programme simplement sans communiquer avec
            le serveur web afin de voir comment l'analyse vocale
            fonctionne il suffit de le lancer avec comme url la chaîne
            <code>'null'</code>. Le petit
            script <code>trywords.sh</code> permet de faire des tests
            simplement. Une fois lancé il suffit de parler devant le
            micro. Les mots compris sont affichés sur la sortie
            standard. Le taux de réussite est assez
            statisfaisant. L'expérience montre qu'il dépend beaucoup
            du volume reçu par le micro : en parlant à voix
            suffisamment haute à
            une distance comprise entre 10 et 60 cm la plupart des
            mots sont compris et plus on s'éloigne du micro plus il
            faut parler fort. Le taux de réussite dépend aussi des
            mots : plus ils sont long mieux ils sont décodés, le
            programme fonctionne mieux avec des mots de 3 syllabes
            qu'avec les monosyllables. Dans l'autre sens le décodage
            peut se déclencher sur les bruits s'ils sont forts : le
            fait d'éternuer à 3 mètres du micro peut activer une commande.
          </p>
          <p>
            Le programme <code>whatusay</code> peut communiquer avec
            le serveur web qui gère les scripts de contrôle de la
            radio. L'url du serveur est par default
            <code>http://localhost:18000</code> mais peut être
            spécifiée différemment sur la ligne de commande. On peut
            donc contrôller l'application en faisant aussi tourner
            <code>whatusay</code> sur une autre machine que la
            raspberry. Le programme génère la requête 
            <code>/vox/process/<i>mot</i></code> pour le serveur à
            chaque traitement d'une parole, <code>mot</code> étant la
            transcription de la parole comprise.
          </p>
          <p>
            Le code du server pour traiter les requêtes de commandes
            vocales se trouve dans le fichier <code>vox.js</code> dans
            le sous-répertoire <code>www/kontrol/server</code>. Les
            mots connus sont regroupés en liste de synonymes,
            c'est-à-dire conduisant à la même opération. Cela donne
            une certaine souplesse pour tester différentes
            expressions. Certains mots sont mieux compris ou plus
            faciles à prononcer pour un non-anglophone.
          </p>
          <p>
            Les commandes comprises sont les suivantes:
            <ul>
            <li>démarrer la radio :
              <code><b>music, wake up, begin, radio</b>, play, run, start</code></li>

            <li>arrêter la radio :
              <code><b>terminate, shut up, silence</b>, cancel, halt,
                stop, quiet, sleep</code></li>

            <li>changer la station :
              <code><b>first, last, next, previous</b></code></li>

            <li>choisir la station :
              <code><b>zero, one, two, three, four, five,
                six, seven</b></code></li>

            <li>augmenter le volume :
              <code>more, louder, plus, higher</code></li>

            <li>diminuer le volume : 
              <code>less, softer, minus, lower</code></li>
            </ul>
            Les mots en gras sont les mieux compris et donc les plus
            utilisés. À noter cependant que cette page de
            documentation n'est pas forcèment synchronisé avec le
            contenu du fichier mis sur github.
          </p>

          <p>
            La méthode <code>app.get</code> extrait le paramètre
            <code>word</code> et regarde dans quel groupe de synomymes
            il est présent puis exécute le shell script qui
            correspond. La reconnaissance vocale est loin d'être
            fiable à 100% aussi il est important de donner un feed
            back pour informer l'utilisateur si la commande a été
            comprise ou non.
            Ce retour est lui aussi sous forme audio. Des phrases ont
            été enregistrées et sauvegardées dans des fichiers
            <code>.wav</code>. Elles sont jouées par la commande
            <code>aplay(1)</code> après la prise en compte des
            requêtes vocales.
          </p>
          <p>
            Un des problèmes à résoudre pour la mise en place de cette
            fonctionnalité est le réglage des volumes relatifs du son
            produit par la radio via <code>mplayer</code> et du son de
            feed back produit par <code>aplay</code>. Pour que ce feed
            back serve à quelque chose son volume doit être légèrement
            supérieur à celui de la radio afin d'être entendu
            distinctement.  La solution
            consiste à éditer les fichiers de feed back à l'aide du
            programme <a href="http://audacity.sourceforge.net/">audacity</a>.
            Ce programme est très bien fait et très complet. Il permet de
            créer assez facilement les fichiers son dont on a besoin,
            de les modifier à sa guise et de régler l'amplitude des
            signaux enregistrés.
          </p>
          <p>
            Sur le GitHub de <em class="rk">RadioK</em> dans le
            répertoire
            <a href=
               "https://github.com/jplf/radiok/tree/master/lib/sounds">
              <code>sounds</code></a> on trouve les fichiers son
              utilisés pour le feed back.
          </p>
          <p>
            Cette version fonctionne relativement correctement. Les
            performances sur la raspberry sont acceptables. Il faut en
            général 2 à 3 secondes pour être compris. Cependant
            l'utilisation de l'anglais comme langue implique une bonne
            prononciation, en particulier un effort sur l'accent
            tonique. Par ailleurs l'application peut se déclencher sur
            des bruits parasites : il faut mieux éviter d'éternuer
            trop près du micro et de claquer trop fort une porte.
          </p>

          <h2>La seconde implémentation</h2>
          <p>
            Une deuxième approche a été tentée. Elle se base sur
            l'utilisation du moteur de reconnaissance vocale de google
            qui est disponible en ligne. Avec cette technique on peut
            utiliser le français comme vocabulaire de commande. On
            devrait d'ailleurs pouvoir utiliser n'importe quelle
            langue gérée par google en changeant simplement un
            paramètre lors du lancement du programme. Cette
            implémentation utilise des requêtes http pour communiquer
            avec google, c'est à dire pour envoyer un fichier son et
            recevoir en retour les chaines de caractères résultant de
            l'interprétation par le moteur de google.
          </p>
          <p>
            Le code de cette version se trouve dans le
            répertoire <code>vox/fr</code>. Le programme final
            s'appelle <code>command</code>. Les autres programmes ont
            été écrits pour tester les différentes étapes à coder pour
            arriver au résultat final. L'intégration des différents
            morceaux n'a pas été facile car ils sont écrits en
            utilisant des routines venant de bibliothèques
            différentes, à la documentation limitée et au style de
            programmation particulièrement dégoûtant.
          </p>
          <p>
            La première étape est classique : elle consiste à décoder
            les options données sur la ligne de commande.
            Mais en général les paramètres par défaut sont suffisants pour
            faire fonctionner le programme : on peut le lancer
            simplement en tapant : <code>command</code>.<br>
            En revanche il est indispensable d'avoir défini la
            variable d'environnement <code>GOOGLE_KEY</code>. La
            valeur est fournie par Google quand on s'identifie comme
            utilisateur de l'API - Voir 
            <a href="https://developers.google.com/console/help/new/?hl=fr#usingkeys">
              la documentation</a>.
          </p>
          <p>
            Ensuite on doit initialiser les paramètres
            de <code>curl</code> pour communiquer et avec google et
            avec radiok. Il n'y a rien de très compliqué, il suffit de
            lire la doc de
            <a href="http://curl.haxx.se/libcurl/c/">libcurl</a>.
          </p>
          <p>
            Une fois les initialisations effectuées on entre dans la
            boucle permanente pour écouter et tenter de reconnaître
            les paroles prononcées. Pour en sortir on peut dire à
            haute voix "<i>abandon</i>" ou on peut tout simplement
            balancer un Control-C.
          </p>  
          <p>
            Au départ de cette boucle on appelle la
            routine <code>get_utterance()</code> qui détecte les sons
            entendus. Un son borné par des instants de silence est
            stocké sous forme brute dans le tableau
            d'échantillons <code>utterance</code>.
          </p>
          <p>
            Pour passer ces données son à google il va falloir
            d'abord les compresser en utilisant un encodage
            <a href="https://xiph.org/flac/">flac</a>. Cet algorithme est
            adapté aux fichiers son, il compresse les données sans
            perte. L'ennui est que la bibliothèque disponible pour
            manipuler les fichier à compresser est particulièrement 
            mal foutue et mal documentée. Après pas mal d'essais je
            suis finalement parvenu à quelquechose. La routine 
            <code>make_flac_encoder()</code> initialise le traitement
            puis <code>enflac_utterance()</code> effectue la
            compression. Le résultat est mis dans un fichier
            temporaire <code>/tmp/utterance.flac</code> que je n'ai
            pas réussi à éviter.
          </p>
          <p>
            La suite du traitement est codé dans la fonction
            <code>interpret_flac()</code>.
            Le fichier flac est transmis à google en utilisant
            <code>curl_easy_setopt()</code> pour passer son contenu en
            paramètre POST.
            <br>
            La réponse de google est analysée en utilisant la
            fonction <code>json_parse()</code> qui permet de retrouver
            les éléments d'un contenu formatté en json. Finalement, si
            tout s'est bien passé, on obtient un mot en sortie de
            <code>parse_google_content()</code>. Il correspond à
            l'interprétation par google de ce qui a été entendu.
          </p>
          <p>
            Le mot est ensuite passé au serveur de radiok par la
            routine <code>send_command()</code>. Cette fois aussi on
            utilise <code>curl</code> pour communiquer. Comme pour la
            première implémentation le serveur effectue une opération
            choisie en fonction du mot reçu.
          </p>  
          <p>
            Après plusieurs semaines d'utilisation je constate que
            cette implémentation marche très bien et vraiment mieux
            que l'implémentation basée sur un traitement local. En
            revanche il faut un peu plus de temps pour obtenir le
            résultat d'un commande.
          </p>  

          <div class="center" style="margin-top: 2em;">
            <a class="btn btn-primary" href="index.html">Home</a>
          </div>

</div>
      </div>

    </div>
  </div>

  <script
     src="//ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js">
  </script>
  <script
     src="//netdna.bootstrapcdn.com/bootstrap/3.1.1/js/bootstrap.min.js">
  </script>
</body>
</html>
